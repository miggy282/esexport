#!/usr/bin/env python
__author__ = "Miguel Sullivan"
""" Export Elasticsearch data to plain text"""

import sys, json
import requests 
import argparse
import time

def cmd_args():
    parser = argparse.ArgumentParser(description='Export Elasticsearch Logs')
    parser.add_argument('-H', '--host',   help='Elasticsearch host')
    parser.add_argument('-i', '--index',  help='Index name')
    parser.add_argument('-t', '--trange', help='timerange from and to eg dd/mm/YY 00:00:32 to dd/mm/YY/ 00:00:32')
    parser.add_argument('-q', '--query', help='lucene query')
    args = parser.parse_args()

    host = args.host
    query  = args.query
    index = args.index
    ts, te  = args.trange.split("to")

    # Convert time to eppoch
    pattern = '%d/%m/%Y %H:%M:%S'
    t_start = int(time.mktime(time.strptime(ts.strip(), pattern)))
    t_end   = int(time.mktime(time.strptime(te.strip(), pattern)))

    # Return Dict 
    args = {
        'host': host, 
        'index': index, 
        'time_start': t_start, 
        'time_end': t_end, 
        'query': query
    } 

    return args

# Initlise command line arguments 
cmd = cmd_args()
# Build JSON query
payload = json.dumps({
    "facets": {
        "0": {
          "date_histogram": {
            "field": "@timestamp",
            "interval": "5m"
          },
          "global": "true",
          "facet_filter": {
            "fquery": {
              "query": {
                "filtered": {
                  "query": {
                    "query_string": {
                      "query": cmd['query']
                    }
                  },
                  "filter": {
                    "bool": {
                      "must": [
                        {
                          "range": {
                            "@timestamp": {
                              "from": cmd['time_start'],
                              "to": cmd['time_end']
                            }
                          }
                        }
                      ]
                    }
                  }
                }
              }
            }
          }
        }
      },
      "size": 0
})
print payload
# Extract lines based on address ['hits']['hits'][index]['_source']['message']
def extract_lines(data):
    raw = data
    for key in range(len(raw['hits']['hits'])):
       print  json.dumps(raw['hits']['hits'][key]["_source"]["message"]).strip()
    
def scroll_continue(s_id):
    querystring = {'scroll_id': s_id}
    r = requests.get(cmd['host'] + ":9200/_search/scroll?scroll=1m", params=querystring)
    json_data = json.loads(r.text)
    return json_data

if __name__ == '__main__':
    # requests  arguments 
    url = cmd['host'] + ":9200/" + cmd['index'] + "/_search"
    headers = {'Content-Type': "application/json; charset=UTF-8"}
    querystring = {"scroll":"1m"}
    # Initial Request
    try:
       r = requests.post(url, json=payload, headers=headers, params=querystring)
       print r
       s_id  = json.loads(r.text)['_scroll_id']
       hits  = json.loads(r.text)
       # Only scroll if need be
       # If theres hits in the initial request we're going to extract them 
       if hits:
          extract_lines(hits)
          # Contintue scrolling if theres more hits
          while True:
             try:
                # Return _scroll results to variable
                hits = scroll_continue(s_id)
             except requests.exceptions.RequestException as e:
                print  "Error: {}".format(e), "Failed"
                sys.exit(1)
             if 'hits' in hits:
                extract_lines(hits)
             else: 
                # Scrolls should be explicitly cleared as soon as the scroll is not being used anymore 
                payload = {"scroll":s_id}
                requests.delete(cmd['host'] + ":9200/" + "_search/scroll", data=payload, headers=headers)
                print "Completed! End of logs"
                break
                sys.exit(0)
    except requests.exceptions.RequestException as e:
       print  "Error: {}".format(e), "Failed"
   
